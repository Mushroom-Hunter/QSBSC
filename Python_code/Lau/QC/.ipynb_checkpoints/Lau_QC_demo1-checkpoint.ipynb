{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from pydpc import Cluster\n",
    "%matplotlib inline\n",
    "import scipy.io "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lib_data(object):\n",
    "    \n",
    "    def size_rank(self): #this needed to be defined before the __init__ because it needs to be run during the __init__ if the initializing data is in dataframe format\n",
    "        self.lib_size = np.sum(self.lib_counts,axis=1) #calculate \n",
    "        self.lib_rank = np.argsort(self.lib_size)[::-1]\n",
    "        self.mt_mouse_mask = pd.Series( self.lib_counts.columns).str.contains('mt-').values # startswith() is case sensitive\n",
    "        self.mt_human_mask = pd.Series( self.lib_counts.columns).str.contains('MT-').values # startswith() is case sensitive\n",
    "        self.lib_rank_inflec = None \n",
    "        \n",
    "    def __init__(self,hdf5_file = None,lib_counts = None):\n",
    "        self.hdf5 = hdf5_file\n",
    "        self.lib_counts = lib_counts #main array of count data\n",
    "        if (hdf5_file == None):\n",
    "            self.size_rank()\n",
    "\n",
    "    def hdf5_to_counts(self):\n",
    "        #need to put some kind of checker here to make sure that first index of datasets is actually counts and not vice versa \n",
    "        self.lib_counts=pd.DataFrame(self.hdf5['Top_Counts'].value)\n",
    "        transcriptsIn=pd.DataFrame(self.hdf5['Transcripts'].value)\n",
    "        barcodesIn=pd.DataFrame(self.hdf5['Barcodes'].value)\n",
    "        transcriptsIn=list(transcriptsIn[0].apply(lambda x: x.decode('utf-8'))) #decode bytes to utf-8\n",
    "        barcodesIn=list(barcodesIn[0].apply(lambda x: x.decode('utf-8'))) #decode bytes to utf-8\n",
    "        self.lib_counts.columns=transcriptsIn\n",
    "        self.lib_counts.index=barcodesIn\n",
    "        self.size_rank()\n",
    "        \n",
    "    def marker_percent(self,mask):\n",
    "        marker_sum = np.sum( self.lib_counts.iloc[:,mask].values, axis=1)\n",
    "        return marker_sum/self.lib_size.astype(float)*100 #percent of counts that are mitochondrial given all counts in cell\n",
    "           \n",
    "    def find_inflection(self):\n",
    "        \n",
    "        cumsum=np.cumsum(self.lib_size)\n",
    "        length=len(cumsum)\n",
    "        x_vals=np.arange(0,length)\n",
    "        secant_coef=cumsum[length-1]/length\n",
    "        secant_line=secant_coef*x_vals\n",
    "        secant_dist=abs(cumsum-secant_line)\n",
    "        max_dist=np.where(np.max(secant_dist)==secant_dist)[0][0]\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(np.array(cumsum), label=\"Cumulative Sum\")\n",
    "        plt.plot(np.array(secant_line), label=\"Secant Line\")\n",
    "        plt.plot(np.array(secant_dist), label=\"CS-SL Distance\")\n",
    "        plt.axvline(x=max_dist,c='red',linestyle='--',linewidth=1,label=\"Inflection Point\")\n",
    "        plt.legend()\n",
    "        self.lib_rank_inflec = np.sort(np.where(self.lib_rank<max_dist)).ravel()\n",
    "        print(\"Inflection point at {}\".format(max_dist))\n",
    "        \n",
    "    def top_n_lib_counts(self,inflec):\n",
    "        top_n_output = self.lib_counts.iloc[inflec,:]\n",
    "        return top_n_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dimension_reduction(object):\n",
    "    \n",
    "    def __init__(self,lib_data_in,seed = 42):\n",
    "        self.lib_data = lib_data_in.lib_counts\n",
    "        self.lib_size = lib_data_in.lib_size\n",
    "        self.lib_rank = lib_data_in.lib_rank\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def normalize_arcsinh(self,cofactor):\n",
    "        self.lib_data = np.arcsinh(self.lib_data.div(self.lib_size.values,axis = 'rows')*cofactor)\n",
    "        \n",
    "    def runPCA(self,n_pcs):\n",
    "        _pca = PCA(n_components = n_pcs)\n",
    "        self.PCA = _pca.fit(self.lib_data).transform(self.lib_data)\n",
    "    \n",
    "    def runUMAP(self,n_neighbors = 10,min_dist = 0.5):\n",
    "        self.UMAP = umap.UMAP(n_neighbors = n_neighbors, min_dist = min_dist, metric = 'correlation').fit_transform(self.PCA)\n",
    "        \n",
    "    def runTSNE(self,n_components=2):\n",
    "        self.TSNE = TSNE(n_components=n_components,n_iter=5000,metric='euclidean',init='pca',random_state=self.seed).fit_transform(self.PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gate_thresh(object):\n",
    "    \n",
    "    def __init__(self,dr_in):\n",
    "        self.PCA = dr_in.PCA\n",
    "        #self.UMAP = dr_in.UMAP\n",
    "        self.TSNE = dr_in.TSNE\n",
    "        self.seed = dr_in.seed\n",
    "        self.lib_rank = dr_in.lib_rank\n",
    "        self.DPC = None\n",
    "        \n",
    "    def plotPCA(self):\n",
    "        fig = plt.figure(figsize=(16,16))\n",
    "        ax1 = fig.add_subplot(221)\n",
    "        ax2 = fig.add_subplot(222)\n",
    "        ax3 = fig.add_subplot(223)\n",
    "        ax4 = fig.add_subplot(224)\n",
    "\n",
    "        ax1.scatter(self.PCA[:,0],self.PCA[:,1],alpha=0.5, s=20)\n",
    "        ax2.scatter(self.PCA[:,1],self.PCA[:,2],alpha=0.5, s=20)\n",
    "        ax3.scatter(self.PCA[:,2],self.PCA[:,3],alpha=0.5, s=20)\n",
    "        ax4.scatter(self.PCA[:,3],self.PCA[:,4],alpha=0.5, s=20)\n",
    "\n",
    "        ax1.set_xlabel( \"PC_0\")\n",
    "        ax1.set_ylabel( \"PC_1\")\n",
    "        ax2.set_xlabel( \"PC_1\")\n",
    "        ax2.set_ylabel( \"PC_2\")\n",
    "        ax3.set_xlabel( \"PC_2\")\n",
    "        ax3.set_ylabel( \"PC_3\")\n",
    "        ax4.set_xlabel( \"PC_3\")\n",
    "        ax4.set_ylabel( \"PC_4\")\n",
    "    \n",
    "    def plotUMAP(self,marker_overlay):\n",
    "        cmap_rank = plt.cm.get_cmap('seismic_r')\n",
    "        cmap_marker = plt.cm.get_cmap('viridis')\n",
    "        cmap_clust=plt.cm.get_cmap('tab20', len(self.DPC.clusters))\n",
    "\n",
    "        fig = plt.figure(figsize=(30,10))\n",
    "        ax1 = fig.add_subplot(131)\n",
    "        ax2 = fig.add_subplot(132)\n",
    "        ax3 = fig.add_subplot(133)\n",
    "        \n",
    "        rank = ax1.scatter(self.UMAP[:,0],self.UMAP[:,1],alpha=1, s=20,c = self.lib_rank,cmap = cmap_rank)\n",
    "        mito = ax2.scatter(self.UMAP[:,0],self.UMAP[:,1],alpha=1, s=20,c = marker_overlay,cmap = cmap_marker)\n",
    "        clust = ax3.scatter(self.UMAP[:,0],self.UMAP[:,1],alpha=1, s=20,c = self.DPC.membership,cmap = cmap_clust) \n",
    "        \n",
    "        plt.colorbar(rank, ax = ax1)\n",
    "        plt.colorbar(mito, ax = ax2)\n",
    "        plt.colorbar(clust, ax = ax3,ticks=range(len(self.DPC.clusters)))\n",
    "        \n",
    "        ax1.set_xlabel(\"Ranking\")\n",
    "        ax2.set_xlabel(\"Marker Enrichment\")\n",
    "        ax3.set_xlabel(\"Density Peak Clustering\")\n",
    "    \n",
    "    def plotTSNE(self,marker_overlay):\n",
    "        cmap_rank = plt.cm.get_cmap('seismic_r')\n",
    "        cmap_marker = plt.cm.get_cmap('viridis')\n",
    "        cmap_clust=plt.cm.get_cmap('tab20', len(self.DPC.clusters))\n",
    "\n",
    "        fig = plt.figure(figsize=(30,10))\n",
    "        ax1 = fig.add_subplot(131)\n",
    "        ax2 = fig.add_subplot(132)\n",
    "        ax3 = fig.add_subplot(133)\n",
    "        \n",
    "        rank = ax1.scatter(self.TSNE[:,0],self.TSNE[:,1],alpha=1, s=20,c = self.lib_rank,cmap = cmap_rank)\n",
    "        mito = ax2.scatter(self.TSNE[:,0],self.TSNE[:,1],alpha=1, s=20,c = marker_overlay,cmap = cmap_marker)\n",
    "        clust = ax3.scatter(self.TSNE[:,0],self.TSNE[:,1],alpha=1, s=20,c = self.DPC.membership,cmap = cmap_clust) \n",
    "        \n",
    "        plt.colorbar(rank, ax = ax1)\n",
    "        plt.colorbar(mito, ax = ax2)\n",
    "        plt.colorbar(clust, ax = ax3,ticks=range(len(self.DPC.clusters)))\n",
    "        \n",
    "        ax1.set_xlabel(\"Ranking\")\n",
    "        ax2.set_xlabel(\"Marker Enrichment\")\n",
    "        ax3.set_xlabel(\"Density Peak Clustering\")\n",
    "    \n",
    "    def runDPC(self,dr,x_cutoff,y_cutoff,force_rerun = False):\n",
    "        if ((self.DPC == None) or (force_rerun == True)):\n",
    "            self.DPC = Cluster(dr.astype('float64'))\n",
    "        self.DPC.assign(x_cutoff,y_cutoff)\n",
    "        \n",
    "    def manual_gating(self,gate_out):\n",
    "        clust_inds = np.delete(np.arange(0,len(self.DPC.membership),1),gate_out) # clusters that represent cells to keep\n",
    "        clust_mask = np.isin(self.DPC.membership, clust_inds)\n",
    "        gated_embedding = self.TSNE[clust_mask]\n",
    "        gated_ranking = self.lib_rank[clust_mask]\n",
    "\n",
    "        cmap_rank = plt.cm.get_cmap('seismic_r')\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        rank = ax1.scatter(gated_embedding[:,0],gated_embedding[:,1],alpha=1,s=20,c = gated_ranking,cmap = cmap_rank)\n",
    "        plt.colorbar(rank, ax = ax1)\n",
    "        \n",
    "        ax1.set_xlabel(\"Gated Ranking\")\n",
    "        \n",
    "        return (clust_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def library_sorting(data_df): #Library sorting function by Sarah Maddox\n",
    "    return(data_df.reindex(data_df.sum(axis = 1).sort_values().index, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and data from .csv file and initializing lib_data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataIn=pd.read_csv(\"https://www.dropbox.com/s/y1iz4rt67s4e8es/Example_dataset.csv?dl=1\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataIn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataInSorted = library_sorting(dataIn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pass it directly into the object for initialization, This will temporarily spike RAM usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indrops_data=lib_data(lib_counts=dataInSorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we check to see that the dataframe was passed properly into the object, it should look identical to the raw dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indrops_data.lib_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the object has been properly initialized, everything else can be performed the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inflection Point Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to perform inflection point thresholding as a low cost/high performance method to quickly select for the cells with libraries that contribute significantly to the cumulative counts of the entire dataset.\n",
    "\n",
    "The current inflection point finding method assumes that the libraries have been sorted by size: greatest to least or least to greatest, the inflection should be detectable. Future versions will include sorting methods.\n",
    "\n",
    "Note that sometimes a fair amount of leeway should be included in inflection point definition. As a more generous threshold (which includes a handful of lower count libraries), can often capture rarer cell populations whose quality will be controlled for in downstream gating based analyses. For example, though our algorithm defines the inflection point as 1608, we could set it at 1500 to capture 500 cells who may have some interesting properties.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indrops_data.find_inflection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are storing the initial library size information for comparative analyses later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_size_before_processing=indrops_data.lib_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Inflection Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to initialize new QC object with only the top libraries based on inflection point, the libraries to include will depend on how the libraries have been sorted. \n",
    "\n",
    "For example, the example dataset here was originally sorted from least to greatest library size, thus we take the last 1500 or so libraries in the dataset (as determined by the inflection point, 2000 - 1500)\n",
    "\n",
    "A dataset sorted from greatest to least, would likely have an inflection point at 1500 or so, and we would take the first 1500 libraries instead of the last.\n",
    "\n",
    "The lib_data analysis object is initialized using a pandas DataFrame containing the counts, gene names, and cell barcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indrops_data=lib_data(lib_counts=indrops_data.lib_counts.tail(len(indrops_data.lib_rank)-1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indrops_data.lib_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reductions for Downstream Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the lib_data object, indrops_data in this example, to initialize another object for data normalization and dimensionality reductions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indrops_dr=dimension_reduction(indrops_data,seed=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, only an arcsinh based transform and library size normalization is implemented. \n",
    "\n",
    "The cofactor, 1000 in this example, affects the linearity of the resulting transformed data. Any values within this range will scale linearly, while values outside of this range will become more log-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indrops_dr.normalize_arcsinh(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linearly reducing the dimensions with PCA enables faster downstream processing with other algorithms such as UMAP or tSNE, here we use 100 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indrops_dr.runPCA(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indrops_dr.runTSNE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and Gating Cell Subpopulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the dimension_reduction object, indrops_dr, to initialize the clustering and gating based quality control. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indrops_gt=gate_thresh(indrops_dr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of visualization, we generate overlays based on the percent of counts/cell meeting some criteria defined by a mask, in this case human and mouse mitochondrial genes comprise masks built into the indrops_data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_mt_pct=indrops_data.marker_percent(indrops_data.mt_human_mask)\n",
    "mouse_mt_pct=indrops_data.marker_percent(indrops_data.mt_mouse_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we identify clusters the dimension reduced space, currently only Density Peak Clustering (Rodriguez et al.) is implemented in this pipeline. This is because it is intuitive to use, fast, and can detect non-spherical clusters similar to DBSCAN. Primarily this clustering step is simply to segment the data into gate-able subpopulations.\n",
    "\n",
    "The points on the decision plot represent cells that act as cluster centers, and are separated based on delta/AU and density. Delta/AU represents the distance between the current cell and any other cell with higher density values. The ratio between this metric provide an intuitive definition of putative clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indrops_gt.runDPC(indrops_gt.TSNE,5,2) #the last two values are the x,y cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indrops_dr.lib_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the overlays, ordinal library quality, and DPC defined clusters. \n",
    "\n",
    "This example dataset contains both human and mouse transcripts, so we must visualize using both our mouse and human mitochondrial overlays that we defined earlier.\n",
    "\n",
    "With respect to library quality, the higher the rank value, the lower the quality, as they were ranked from highest to lowest library sizes with an argsort. \n",
    "\n",
    "The third set of plots simply demonstrates that other marker overlays can also be used to pinpoint the expression of genes of interest, Igf2 is visualized here for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indrops_gt.plotTSNE(mouse_mt_pct) \n",
    "indrops_gt.plotTSNE(human_mt_pct)\n",
    "indrops_gt.plotTSNE(indrops_dr.lib_data['mm10_Igf2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must gate out low quality or dead cells as defined by library size and the aggregate mitochondrial markers (mouse and human respectively)\n",
    "\n",
    "Here we gate out the following subpopulations:\n",
    "\n",
    "0, 1 and, 2 for high mouse and human mitochondrial expression\n",
    "\n",
    "6 for low library size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_indices=indrops_gt.manual_gating([0,1,2,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the indices of the desired cell subpopulations, we initialize a new lib_data object with the new subset of cells, this is because we need to perform another set of dimensionality reductions to properly visualize the data after having changed its global structure by removing undesireable cell subpopulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_indrops_data=lib_data(lib_counts=indrops_data.lib_counts[gated_indices]) #make new gated object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of example, here we generate a couple new masks/overlays that will show human vs. mouse transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_gene_mask=pd.Series(gated_indrops_data.lib_counts.columns.str.contains(\"mm10_\")).values\n",
    "human_gene_mask=pd.Series(gated_indrops_data.lib_counts.columns.str.contains(\"hg19_\")).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_pct_gated=gated_indrops_data.marker_percent(human_gene_mask)\n",
    "mouse_pct_gated=gated_indrops_data.marker_percent(mouse_gene_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-running dimensionality reduction and visualizing QC'd Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the procedure of initializing a dimension_reduction object, the normalizations/transformations that take place, and the dim. reductions but this time with just the cells of interest remaining after our first gating procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_indrops_dr=dimension_reduction(gated_indrops_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_indrops_dr.normalize_arcsinh(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_indrops_dr.runPCA(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_indrops_dr.runTSNE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_indrops_gt=gate_thresh(gated_indrops_dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_indrops_gt.runDPC(gated_indrops_gt.TSNE,5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_indrops_gt.plotTSNE(human_pct_gated)\n",
    "gated_indrops_gt.plotTSNE(mouse_pct_gated)\n",
    "gated_indrops_gt.plotTSNE(gated_indrops_dr.lib_data['mm10_Igf2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If things are up to par, we can go ahead and write this new set of points to a file.\n",
    "\n",
    "This part is up to the user's discretion, as each experiment will necessitate different populations of cells, which can be pinpointed through overlays of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final check visualizing dataset library quality at each step of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,8))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.hist(lib_size_before_processing,bins=100)\n",
    "ax2.hist(indrops_data.lib_size,bins=100)\n",
    "ax3.hist(gated_indrops_data.lib_size,bins=100)\n",
    "\n",
    "ax1.set_title(\"Before Processing\")\n",
    "ax1.set_ylabel(\"Number of Cells\")\n",
    "ax1.set_xlabel(\"Number of UMIs\")\n",
    "ax2.set_title(\"After Inflection\")\n",
    "ax2.set_xlabel(\"Number of UMIs\")\n",
    "ax3.set_title(\"After Inflection+Gating\")\n",
    "ax3.set_xlabel(\"Number of UMIs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=pd.DataFrame([gated_indrops_data.lib_size.describe(),indrops_data.lib_size.describe(),lib_size_before_processing.describe()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.index=['UMIs After_Inflection+Gating','UMIs After_Inflection','UMIs Before_Processing']\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just have some general information about how the dataset has changed at each step of QC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do a final check on the QC'd library in preparation for writing it to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_indrops_data.lib_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check which directory we're in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and, write the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_indrops_data.lib_counts.to_csv(\"Quality_Controlled_Indrops_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
